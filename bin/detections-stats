#! /usr/bin/env python
import argparse
import logging
from importlib import resources
from math import inf
from pathlib import Path
from time import perf_counter as timer

from fastavro import reader
from fastavro.schema import load_schema
from obspy import UTCDateTime
from tqdm import tqdm

import correlation_detector as cd

parser = argparse.ArgumentParser(prog="detections-stats")
parser.add_argument("catalog", help="Path to event catalog, location of the zmap file", type=Path)
parser.add_argument("input", help="Input file path, location of the avro file", type=Path)
parser.add_argument("output", help="Output file path, stem of the output cat and stats file", type=Path)
parser.add_argument("--threshold", help="Correlation threshold per channels", type=float, default=0.4)
parser.add_argument("--min_channels", help="Minimum number of channels with correlation above threshold",
                    type=int, default=7)
parser.add_argument("--mag_relative_threshold", help="Scale factor of channel magnitude MAD threshold", type=float,
                    default=2.0)
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--sort", help="Sort output by template and date", default=False, action='store_true')
parser.add_argument("--progress", help="Show progress bar", default=False, action='store_true')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

with resources.path('correlation_detector', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)

if __name__ == '__main__':
    logging.info(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    template_magnitudes = cd.read_zmap(cli_args.catalog)
    logging.info(f"Reading from {cli_args.input}")
    with cli_args.input.open('rb') as avro:
        events_buffer = []
        detections = reader(avro, reader_schema=schema)
        for detection in tqdm(detections, disable=not cli_args.progress):
            channels = detection['channels']
            num_channels = sum(1 for channel in channels if channel['correlation'] > cli_args.threshold)
            if num_channels > cli_args.min_channels:
                timestamp = UTCDateTime(detection['timestamp'])
                height = sum(channel['height'] for channel in channels) / len(channels)
                correlation = sum(channel['correlation'] for channel in channels) / len(channels)
                template_magnitude = template_magnitudes.iloc[detection['template'] - 1]
                magnitude = cd.estimate_magnitude(template_magnitude, [channel['magnitude'] for channel in channels],
                                                  cli_args.mag_relative_threshold)
                dmad = detection['dmad']
                detection.update({'datetime': timestamp, 'height': height, 'correlation': correlation,
                                  'magnitude': magnitude, 'template_magnitude': template_magnitude,
                                  'channels': channels, 'num_channels': num_channels,
                                  'crt_pre': inf if dmad == 0.0 else height / dmad,
                                  'crt_post': inf if dmad == 0.0 else correlation / dmad})
                for name, ref in [('30%', 0.3), ('50%', 0.5), ('70%', 0.7), ('90%', 0.9)]:
                    detection[name] = sum(1 for channel in channels if channel['correlation'] > ref)
                events_buffer.append(detection)
        if cli_args.sort:
            events_buffer.sort(key=lambda event: (event['template'], event['timestamp']))
    logging.info(f"Writing {len(events_buffer)} {'events' if len(events_buffer) > 1 else 'event'} "
                 f"to {cli_args.output}.cat and {cli_args.output}.stats")
    cat = cli_args.output.with_suffix('.cat')
    if cat.exists():
        logging.info(f"{cat} already exists, it will be overwritten.")
    with cat.open('w') as file:
        file.writelines(cd.format_cat(event) for event in events_buffer)
    stats = cli_args.output.with_suffix('.stats')
    if stats.exists():
        logging.info(f"{stats} already exists, it will be overwritten.")
    with stats.open('w') as file:
        file.writelines(cd.format_stats(event) for event in events_buffer)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds.")
