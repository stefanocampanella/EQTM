#! /usr/bin/env python
import argparse
import logging
from importlib import resources
from itertools import repeat
from pathlib import Path
from time import perf_counter as timer

import pandas as pd
from fastavro import reader
from fastavro.schema import load_schema

import correlation_detector as cd

parser = argparse.ArgumentParser(prog=__file__)
parser.add_argument("catalog", help="Path to event catalog, location of the zmap file", type=Path)
parser.add_argument("ttimes", help="Path to travel times directory", type=Path)
parser.add_argument("input", help="Input file path, location of the Avro file", type=Path)
parser.add_argument("output", help="Output file path, stem of the Parquet file", type=Path)
parser.add_argument("--corr_atol", help="Absolute tolerance above which to consider correlations as errors",
                    type=float, default=1e-3)
parser.add_argument("--query", help="Optional query to select data from DataFrame", type=str, default=None)
parser.add_argument("--log", help="Log level", default='info')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

with resources.path('correlation_detector', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)

if __name__ == '__main__':
    logging.debug(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    logging.info(f"Reading from {cli_args.input.resolve()}")
    with cli_args.input.open('rb') as file:
        events_buffer = sorted(map(cd.fix_correlations, reader(file, reader_schema=schema), repeat(cli_args.corr_atol)),
                               key=lambda event: event['timestamp'])
    logging.info(f"Found {len(events_buffer)} {'events' if len(events_buffer) > 1 else 'event'}")
    zmap = cd.read_zmap(cli_args.catalog)
    records = map(lambda event: cd.make_record(event, zmap, cli_args.ttimes), events_buffer)
    catalogue = pd.DataFrame.from_records(records)
    catalogue['date'] = pd.to_datetime(catalogue['timestamp'], unit='s', utc=True)
    catalogue.set_index('date', inplace=True, verify_integrity=True)
    if cli_args.query:
        logging.info(f"Querying: \"{cli_args.query}\"")
        catalogue.query(cli_args.query, inplace=True)
    output = cli_args.output.with_suffix('.parquet')
    if output.exists():
        logging.info(f"{output} already exists, it will be overwritten")
    logging.info(f"Writing {len(catalogue)} {'events' if len(catalogue) > 1 else 'event'} to {output.resolve()}")
    catalogue.to_parquet(output, engine='fastparquet', index=True)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds")
