#! /usr/bin/env python
import argparse
import logging
import pandas as pd
from fastavro import reader
from fastavro.schema import load_schema
from importlib import resources
from itertools import repeat
from pathlib import Path
from time import perf_counter as timer

import eqtm

parser = argparse.ArgumentParser(prog=__file__)
parser.add_argument("catalog", help="Path to event catalog, location of the zmap file", type=Path)
parser.add_argument("ttimes", help="Path to travel times directory", type=Path)
parser.add_argument("input", help="Input file path, location of the Avro file", type=Path)
parser.add_argument("output", help="Output file path, stem of the Parquet file", type=Path)
parser.add_argument("--corr_atol", help="Absolute tolerance above which to consider correlations as errors",
                    type=float, default=1e-3)
parser.add_argument("--keep_duplicates", help="Keep events with the same date (may cause duplicated indices)",
                    choices=['all', 'none', 'best'], default='first')
parser.add_argument("--verify_integrity", help="Verify that dataframe has unique indices (may affect performance)",
                    default=False, action='store_true')
parser.add_argument("--log", help="Log level", default='info')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

with resources.path('eqtm', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)

if __name__ == '__main__':
    logging.debug(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    logging.info(f"Reading from {cli_args.input.resolve()}")
    with cli_args.input.open('rb') as file:
        events_buffer = sorted(
            map(eqtm.fix_correlations, reader(file, reader_schema=schema), repeat(cli_args.corr_atol)),
            key=lambda event: event['timestamp'])
    logging.info(f"Found {len(events_buffer)} {'events' if len(events_buffer) > 1 else 'event'}")
    zmap = eqtm.read_zmap(cli_args.catalog)
    records = map(lambda event: eqtm.make_record(event, zmap, cli_args.ttimes), events_buffer)
    catalogue = pd.DataFrame.from_records(records)
    catalogue['date'] = pd.to_datetime(catalogue['timestamp'], unit='s', utc=True)
    if cli_args.keep_duplicates == 'none':
        catalogue.drop_duplicates(subset=['date'], keep=False, inplace=True)
    elif cli_args.keep_duplicates == 'best':
        catalogue.sort_values(by=['date', 'correlation_mean'], ascending=[True, False], inplace=True)
        catalogue.drop_duplicates(subset=['date'], keep='first', inplace=True)
    catalogue.set_index('date', inplace=True, verify_integrity=cli_args.verify_integrity)
    output = cli_args.output.with_suffix('.parquet')
    if output.exists():
        logging.info(f"{output} already exists, it will be overwritten")
    logging.info(f"Writing {len(catalogue)} {'events' if len(catalogue) > 1 else 'event'} to {output.resolve()}")
    catalogue.to_parquet(output, engine='fastparquet', index=True)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds")
