#! /usr/bin/env python
import argparse
import logging
from importlib import resources
from itertools import chain
from pathlib import Path
from time import perf_counter as timer

import pandas as pd

from fastavro import reader, writer
from fastavro.schema import load_schema
from tqdm import tqdm

parser = argparse.ArgumentParser(prog="cat-detections")
parser.add_argument("input", help="Input directory", type=Path)
parser.add_argument("output", help="Output file path", type=Path)
parser.add_argument("--hdf", help="Concat HDF5 format", default=False, action='store_true')
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--progress", help="Show progress bar", default=False, action='store_true')
parser.add_argument("--stop", help="Stop if an error occurs", default=False, action='store_true')
parser.add_argument("--compression", help="Compression algorithm to use", choices=['zlib', 'lzo', 'bzip2', 'blosc'],
                    default='blosc')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

with resources.path('correlation_detector', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)


def get_avro_data(source, stop):
    try:
        with source.open('rb') as file:
            logging.debug(f"Reading {file.name}")
            for row in reader(file, reader_schema=schema):
                yield row
    except BaseException as exception:
        if stop:
            raise exception
        else:
            logging.warning(f"An error occurred while reading {source}", exc_info=exception)


if __name__ == '__main__':
    logging.info(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    logging.info(f"Reading from {cli_args.input}")
    output = cli_args.output.with_suffix('.h5' if cli_args.hdf else '.avro')
    if output.exists():
        logging.info(f"{output} already exists, it will be overwritten.")
    if cli_args.hdf:
        logging.info(f"Writing to {output.name}")
        catalogue = pd.HDFStore(output, mode='w', complib=cli_args.compression)
        paths = sorted(cli_args.input.glob('*.h5'))
        if cli_args.progress:
            paths = tqdm(paths)
        for path in paths:
            catalogue.append('catalogue', pd.read_hdf(path))
        catalogue.close()
    else:
        rows = chain.from_iterable(get_avro_data(source, cli_args.stop) for source in cli_args.input.glob('*.avro'))
        if cli_args.progress:
            rows = tqdm(rows)
        with output.open('wb') as sink:
            logging.info(f"Writing to {sink.name}")
            writer(sink, schema, rows, codec=cli_args.compression)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds.")
