#! /usr/bin/env python
import argparse
import logging
from concurrent.futures import ThreadPoolExecutor
from contextlib import nullcontext
from importlib import resources
from pathlib import Path
from time import perf_counter as timer

import numpy as np
from fastavro import writer
from fastavro.schema import load_schema
from obspy import Stream, UTCDateTime
from psutil import cpu_count
from scipy.signal import find_peaks
from tqdm import tqdm

try:
    import cupy
except ImportError:
    cupy = None

import correlation_detector as cd

parser = argparse.ArgumentParser(prog=__file__)
parser.add_argument("data", help="Continuous data path, location of the mseed file", type=Path)
parser.add_argument("travel_times", help="Path to travel times directory", type=Path)
parser.add_argument("templates", help="Path to templates directory", type=Path)
parser.add_argument("output", help="Output file path, location of the jsonl/avro file", type=Path)
parser.add_argument("--max_channels", help="Maximum number of channels", type=int, default=18)
parser.add_argument("--lowpass", help="Lowpass filter frequency", type=float, default=3.0)
parser.add_argument("--highpass", help="Highpass filter frequency", type=float, default=8.0)
parser.add_argument("--threshold", help="Mean correlation peak threshold", type=float, default=0.4)
parser.add_argument("--distance", help="Scale factor of mean correlation peak distance", type=float, default=2.0)
parser.add_argument("--maxshift", help="Maximum lag in samples", type=int, default=6)
parser.add_argument("--compression", help="Compression algorithm to use", choices=['null', 'deflate', 'snappy'],
                    default='snappy')
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--threads", help="Number of threads to use", type=int, default=0)
parser.add_argument("--progress", help="Show progress bar", default=False, action='store_true')
parser.add_argument("--stop", help="Stop if an error occurs", default=False, action='store_true')
cli_args = parser.parse_args()

logging.basicConfig(format='%(process)s-%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

if cupy:
    # noinspection PyUnresolvedReferences
    streams = [cupy.cuda.Stream() for _ in range(cli_args.max_channels)]
else:
    streams = [nullcontext() for _ in range(cli_args.max_channels)]

with resources.path('correlation_detector', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)

if __name__ == '__main__':
    logging.debug(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    data_pool = cd.read_data(cli_args.data, freqmin=cli_args.lowpass, freqmax=cli_args.highpass)
    templates = cd.read_templatesdata(cli_args.templates, cli_args.travel_times)
    if cli_args.progress:
        templates = tqdm(templates, total=len(list(cli_args.travel_times.glob('*.ttimes'))))
    events_buffer = []
    max_workers = cli_args.threads if cli_args.threads > 0 else max(cpu_count(logical=False), cli_args.max_channels)
    with ThreadPoolExecutor(max_workers=max_workers) as pool:
        for index, template, ttimes in templates:
            try:
                mystery_shift = 2 * min(ttimes.values())  # mystery of faith
                data, template, ttimes = cd.match_traces(data_pool, template, ttimes, cli_args.max_channels)
                correlations = Stream(traces=pool.map(cd.correlate_trace, data, template, ttimes.values(), streams))
                signal = sum(pool.map(lambda trace: cd.max_filter(trace.data, cli_args.maxshift), correlations))
                template_length = max(trace.stats.npts for trace in template)
                peaks, _ = find_peaks(signal, height=cli_args.threshold * len(correlations),
                                      distance=cli_args.distance * template_length)
                data_starttime = UTCDateTime(np.mean([trace.stats.starttime.timestamp for trace in data]))
                template_reference = UTCDateTime(np.mean([trace.stats.starttime.timestamp - ttime
                                                          for trace, ttime in zip(template, ttimes.values())]))
                delta = np.mean([trace.stats.delta for trace in correlations])
                event_shift = mystery_shift + 0.5 * delta
                detections = []
                for peak in peaks:
                    event_date = data_starttime + peak * delta
                    template_shift = event_date - template_reference
                    channels = []
                    for correlation_trace, data_trace, template_trace in zip(correlations, data, template):
                        magnitude = cd.relative_magnitude(data_trace, template_trace, template_shift)
                        height, correlation, shift = cd.max_correlation(correlation_trace, peak, cli_args.maxshift)
                        channels.append(
                            {'id': correlation_trace.id, 'height': height, 'correlation': correlation, 'shift': shift,
                             'magnitude': magnitude})
                    detections.append({'timestamp': (event_date + event_shift).timestamp, 'channels': channels})
                if detections:
                    logging.debug(f"Found {len(detections)} {'events' if len(detections) > 1 else 'event'}")
                    mean_correlation = sum(trace.data for trace in correlations) / len(correlations)
                    dmad = np.median(np.abs(mean_correlation - np.median(mean_correlation)))
                    events_buffer.append({'template': index, 'dmad': dmad, 'detections': detections})
            except BaseException as exception:
                if cli_args.stop:
                    raise exception
                else:
                    logging.warning(f"An error occurred while processing template {index}", exc_info=exception)
    output = cli_args.output.with_suffix('.avro')
    if output.exists():
        logging.info(f"{output} already exists, it will be overwritten")
    with output.open('wb') as file:
        logging.info(f"Writing to {output.resolve()}")
        writer(file, schema, cd.flatten(events_buffer), codec=cli_args.compression)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds")
