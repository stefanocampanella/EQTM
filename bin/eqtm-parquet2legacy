#! /usr/bin/env python
import argparse
import logging
from pathlib import Path
from time import perf_counter as timer

import pandas as pd
from scipy.signal._peak_finding_utils import _select_by_peak_distance

import correlation_detector as cd

parser = argparse.ArgumentParser(prog=__file__)
parser.add_argument("input", help="Input file path, location of the Parquet file", type=Path)
parser.add_argument("distance", help="Minimum distance between events in seconds", type=float)
parser.add_argument("output", help="Output file path, stem of the output cat and stats file", type=Path)
parser.add_argument("--time_unit", help="Precision to use in timestamps rounding", type=str, default='us')
parser.add_argument("--corr_atol", help="Absolute tolerance above which to consider correlations as errors",
                    type=float, default=1e-3)
parser.add_argument("--corr_threshold", help="Correlation threshold per channels", type=float, default=0.4)
parser.add_argument("--mag_threshold", help="Scale factor of channel magnitude MAD threshold", type=float,
                    default=2.0)
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--sort", help="Sort output by template", default=False, action='store_true')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

if __name__ == '__main__':
    logging.debug(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    logging.info(f"Reading from {cli_args.input.resolve()}")
    catalogue = pd.read_parquet(cli_args.input, engine='fastparquet')
    catalogue.reset_index(inplace=True)
    logging.info(f"Found {len(catalogue)} {'events' if len(catalogue) > 1 else 'event'}")
    if cli_args.distance > 0.0:
        logging.info(f"Selecting events at least {cli_args.distance} seconds apart")
        time_precision = pd.to_datetime(1, unit=cli_args.time_unit).timestamp()
        timestamps = (catalogue['timestamp'] / time_precision).astype(int)
        to_keep = _select_by_peak_distance(timestamps.to_numpy(), catalogue['correlation_mean'].to_numpy(),
                                           cli_args.distance / time_precision)
        catalogue = catalogue[to_keep]
    records = [cd.make_legacy_record(event, cli_args.corr_threshold, cli_args.mag_threshold)
               for event in catalogue.to_dict(orient='records')]
    if cli_args.sort:
        records.sort(key=lambda event: (event['template'], event['timestamp']))
    logging.info(f"Writing {len(records)} {'events' if len(records) > 1 else 'event'} "
                 f"to {cli_args.output}.cat and {cli_args.output}.stats "
                 f"in {cli_args.output.parent.resolve()}")
    cat = cli_args.output.with_suffix('.cat')
    if cat.exists():
        logging.info(f"{cat} already exists, it will be overwritten")
    with cat.open('w') as file:
        file.writelines(map(cd.format_cat, records))
    stats = cli_args.output.with_suffix('.stats')
    if stats.exists():
        logging.info(f"{stats} already exists, it will be overwritten")
    with stats.open('w') as file:
        file.writelines(map(cd.format_stats, records))
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds")
