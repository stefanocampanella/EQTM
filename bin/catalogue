#! /usr/bin/env python
import argparse
import logging
import json
from pathlib import Path
from time import perf_counter as timer

import numpy as np
import pandas as pd
from scipy.signal._peak_finding_utils import _select_by_peak_distance

import correlation_detector as cd

parser = argparse.ArgumentParser(prog=__file__)
parser.add_argument("catalog", help="Path to event catalog, location of the zmap file", type=Path)
parser.add_argument("input", help="Input file path, location of the jsonl/avro file", type=Path)
parser.add_argument("output", help="Output file path, stem of the output cat and stats file", type=Path)
parser.add_argument("--distance", help="Minimum distance between events in seconds", type=float, default=6.0)
parser.add_argument("--time_precision", help="Precision to use in timestamps rounding", type=float, default=1e-6)
parser.add_argument("--corr_atol", help="Absolute tolerance in normalized correlation calculations",
                    type=float, default=1e-3)
parser.add_argument("--min_stations", help="Minimum number of channels", type=int, default=3)
parser.add_argument("--corr_threshold", help="Correlation threshold per channels", type=float, default=0.35)
parser.add_argument("--catstats_corr_threshold", help="Correlation threshold per channels", type=float, default=0.4)
parser.add_argument("--catstats_mag_threshold", help="Scale factor of channel magnitude MAD threshold", type=float,
                    default=2.0)
parser.add_argument("--csv", help="Output CSV instead of legacy format", default=False, action='store_true')
parser.add_argument("--hdf", help="Output HDF5 instead of legacy format", default=False, action='store_true')
parser.add_argument("--ttimes", help="Path to travel times directory", type=Path)
parser.add_argument("--network", help="Path to network stations list, location of the network file", type=Path)
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--sort", help="Sort output by template", default=False, action='store_true')
parser.add_argument("--compression", help="Compression algorithm to use", choices=['zlib', 'lzo', 'bzip2', 'blosc'],
                    default='blosc')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

if cli_args.input.name.endswith(".avro"):
    from fastavro import reader
    from fastavro.schema import load_schema

    from importlib import resources

    with resources.path('correlation_detector', 'event.avsc') as schema_path:
        schema = load_schema(schema_path)

    mode = 'rb'
elif cli_args.input.name.endswith(".jsonl"):
    schema = None

    def reader(file, reader_schema=None):
        return (json.loads(line) for line in file.readlines())

    mode = 'r'
else:
    raise RuntimeError("Input file type not recognized")

if __name__ == '__main__':
    logging.info(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    logging.info(f"Reading from {cli_args.input}")
    with cli_args.input.open(mode) as file:
        events_buffer = [cd.clean(event, cli_args.corr_atol, cli_args.min_stations, cli_args.corr_threshold)
                         for event in reader(file, reader_schema=schema) if event]
    logging.info(f"Found {len(events_buffer)} {'events' if len(events_buffer) > 1 else 'event'}")
    if cli_args.distance > 0:
        events_buffer.sort(key=lambda event: event['timestamp'])
        timestamps = np.fromiter((event['timestamp'] / cli_args.time_precision for event in events_buffer), dtype=int)
        correlations = np.fromiter((sum(channel['correlation']
                                        for channel in event['channels']) / len(event['channels'])
                                    for event in events_buffer), dtype=float)
        to_keep = _select_by_peak_distance(timestamps,
                                           correlations,
                                           cli_args.distance / cli_args.time_precision)
        events_buffer = [event for (event, is_to_keep) in zip(events_buffer, to_keep) if is_to_keep]
    if cli_args.sort:
        events_buffer.sort(key=lambda event: (event['template'], event['timestamp']))
    zmap = cd.read_zmap(cli_args.catalog)
    if cli_args.csv or cli_args.hdf:
        output = cli_args.output.with_suffix('.csv' if cli_args.csv else '.h5')
        logging.info(f"Writing {len(events_buffer)} {'events' if len(events_buffer) > 1 else 'event'} to {output.name}")
        records = map(lambda event: cd.make_record(event, zmap, cli_args.network, cli_args.ttimes), events_buffer)
        df = pd.DataFrame.from_records(records)
        if cli_args.csv:
            df.to_csv(output, index=False)
        else:
            df.to_hdf(output, key='catalogue', mode='w', format='table', complib=cli_args.compression, index=False)
    else:
        records = list(map(lambda event: cd.make_legacy_record(event, zmap, cli_args.catstats_corr_threshold,
                                                               cli_args.catstats_mag_threshold),
                           events_buffer))
        logging.info(f"Writing {len(records)} {'events' if len(records) > 1 else 'event'} "
                     f"to {cli_args.output}.cat and {cli_args.output}.stats")
        cat = cli_args.output.with_suffix('.cat')
        if cat.exists():
            logging.info(f"{cat} already exists, it will be overwritten")
        with cat.open('w') as file:
            file.writelines(cd.format_cat(record) for record in records)
        stats = cli_args.output.with_suffix('.stats')
        if stats.exists():
            logging.info(f"{stats} already exists, it will be overwritten")
        with stats.open('w') as file:
            file.writelines(cd.format_stats(record) for record in records)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds")
