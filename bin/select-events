#! /usr/bin/env python
import argparse
import datetime
import logging
from importlib import resources
from pathlib import Path
from time import perf_counter as timer
from concurrent.futures import ProcessPoolExecutor as Executor
from itertools import repeat


import pandas as pd
from fastavro import reader, writer
from fastavro.schema import load_schema
from tqdm import tqdm

parser = argparse.ArgumentParser(prog=__file__)
parser.add_argument("list", help="Path to event catalog, location of the zmap file", type=Path)
parser.add_argument("input", help="Input file path, location of the avro file", type=Path)
parser.add_argument("output", help="Output file path, stem of the output cat and stats file", type=Path)
parser.add_argument("--atol", help="Tolerance in seconds", type=float, default=0.005)
parser.add_argument("--threads", help="Number of threads to use", type=int, default=0)
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--compression", help="Compression algorithm to use", choices=['null', 'deflate', 'snappy'],
                    default='snappy')
parser.add_argument("--progress", help="Show progress bar", default=False, action='store_true')

cli_args = parser.parse_args()

logging.basicConfig(format='%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))


with resources.path('correlation_detector', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)


def filenames_from_date(date):
    filenames = []
    date_string = date.strftime("%Y-%m-%d")
    filenames.append(cli_args.input / (date_string + ".avro"))
    if date.hour == 0:
        daybefore_string = (date - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        filenames.append(cli_args.input / (daybefore_string + ".avro"))
    elif date.hour == 23:
        dayafter_string = (date + datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        filenames.append(cli_args.input / (dayafter_string + ".avro"))
    return filenames


def find_record(template, date, filenames, atol):
    candidates = []
    for filename in filenames:
        with filename.open('rb') as file:
            for event in reader(file, reader_schema=schema):
                if template == event['template']:
                    if abs(date.timestamp() - event['timestamp']) < atol:
                        candidates.append(event)
                    elif date.timestamp() > event['timestamp']:
                        continue
                else:
                    continue
    if candidates:
        return max(candidates,
                   key=lambda x: sum(ch['correlation'] for ch in x['channels']) / len(x['channels']))
    else:
        logging.warning(f"Event at {date.isoformat()} for template {template} not found")
        return None

if __name__ == '__main__':
    logging.info(f"Running {parser.prog} with the following parameters: {vars(cli_args)}")
    tic = timer()
    logging.info(f"Reading from {cli_args.list}")
    selection = pd.read_csv(cli_args.list, delimiter=' ', parse_dates=[1], usecols=[0, 1],
                            names=['template', 'date'])
    filenames_queue = selection['date'].map(filenames_from_date)
        
    max_workers = cli_args.threads if cli_args.threads > 0 else None
    with Executor(max_workers=max_workers) as pool:
        maybe_records = pool.map(find_record, selection['template'], selection['date'], filenames_queue, repeat(cli_args.atol))
        if cli_args.progress:
            maybe_records = tqdm(maybe_records, total=len(selection))
        with open(cli_args.output.with_suffix('.avro'), 'wb') as file:
            logging.info(f"Writing to {file.name}")
            writer(file, schema, filter(None, maybe_records), cli_args.compression)
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds")
