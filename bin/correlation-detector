#! /usr/bin/env python
import argparse
import logging
from concurrent.futures import ThreadPoolExecutor
from importlib import resources
from pathlib import Path
from time import perf_counter as timer

import bottleneck as bn
import numpy as np
from fastavro import writer
from fastavro.schema import load_schema
from obspy import Stream
from psutil import cpu_count
from scipy.signal import find_peaks
from tqdm import tqdm

try:
    import cupy
except ImportError:
    cupy = None

import correlation_detector as cd

parser = argparse.ArgumentParser(prog="correlation-detector")
parser.add_argument("data", help="Continuous data path", type=Path)
parser.add_argument("travel_times", help="Path to travel times directory", type=Path)
parser.add_argument("templates", help="Path to templates directory", type=Path)
parser.add_argument("output", help="Output file path")
parser.add_argument("--max_channels", help="Maximum number of channels", type=int, default=18)
parser.add_argument("--std_filter", help="Scale factor of correlation std MAD threshold", type=float, default=8.0)
parser.add_argument("--peaks_filter", help="Scale factor of peaks MAD threshold", type=float, default=8.0)
parser.add_argument("--lowpass", help="Lowpass filter frequency", type=float, default=3.0)
parser.add_argument("--highpass", help="Highpass filter frequency", type=float, default=8.0)
parser.add_argument("--threshold", help="Mean correlation peak threshold", type=float, default=0.6)
parser.add_argument("--distance", help="Scale factor of mean correlation peak distance", type=float, default=2.0)
parser.add_argument("--tolerance", help="Maximum lag in samples", type=int, default=6)
parser.add_argument("--memory_limit", help="Memory limit in GB (affects only events buffer)", type=float, default=8)
parser.add_argument("--compression", help="Compression algorithm to use", choices=['null', 'deflate', 'snappy'],
                    default='snappy')
parser.add_argument("--log", help="Log level", default='info')
parser.add_argument("--threads", help="Number of threads to use", type=int, default=0)
parser.add_argument("--progress", help="Show progress bar", default=False, action='store_true')
parser.add_argument("--stop", help="Stop if an error occurs", default=False, action='store_true')
cli_args = parser.parse_args()

logging.basicConfig(format='%(process)s-%(levelname)s-%(asctime)s: %(message)s',
                    level=getattr(logging, cli_args.log.upper()))

with resources.path('correlation_detector', 'event.avsc') as schema_path:
    schema = load_schema(schema_path)

if cupy:
    # noinspection PyUnresolvedReferences
    streams = [cupy.cuda.Stream() for _ in range(cli_args.max_channels)]
else:
    streams = [None for _ in range(cli_args.max_channels)]

if __name__ == '__main__':
    tic = timer()
    data_pool = cd.read_data(cli_args.data, freqmin=cli_args.lowpass, freqmax=cli_args.highpass)
    templates = tqdm(cd.read_templates(cli_args.templates, cli_args.travel_times),
                     total=len(list(cli_args.travel_times.glob('*.ttimes'))),
                     disable=not cli_args.progress)
    output = Path(cli_args.output + '.avro')
    if output.exists():
        logging.warning(f"{output} already exists, it will be overwritten.")
        output.unlink()
    events_buffer = []
    max_workers = cli_args.threads if cli_args.threads > 0 else max(cpu_count(logical=False), cli_args.max_channels)
    with ThreadPoolExecutor(max_workers=max_workers) as pool:
        for index, template, ttimes in templates:
            try:
                data, template, ttimes = cd.match_traces(data_pool, template, ttimes, cli_args.max_channels)
                correlations = Stream(traces=pool.map(cd.correlate_trace, data, template, ttimes.values(), streams))
                if correlations:
                    stds = np.fromiter(pool.map(lambda trace: bn.nanstd(np.abs(trace.data)), correlations), dtype=float)
                    cd.filter_data(stds, correlations, data, template, ttimes, mad_factor=cli_args.std_filter)
                    shifted_correlations = Stream(traces=pool.map(lambda trace: cd.max_filter(trace.data,
                                                                                              cli_args.tolerance),
                                                                  correlations))
                    mean_correlation = bn.nanmean(shifted_correlations, axis=0)
                    dmad = bn.nanmedian(np.abs(mean_correlation - bn.nanmedian(mean_correlation)))
                    peaks, _ = find_peaks(mean_correlation, height=cli_args.threshold,
                                          distance=cli_args.distance * max(trace.stats.npts for trace in template))
                    peaks = cd.filter_peaks(peaks, shifted_correlations, cli_args.threshold, cli_args.peaks_filter)
                    detections = cd.process_detections(peaks, correlations, data, template, ttimes, pool,
                                                       cli_args.tolerance)
                    for detection in detections:
                        detection.update({'template': index, 'dmad': dmad})
                        events_buffer.append(detection)
            except BaseException as error:
                if cli_args.stop:
                    raise error
                else:
                    logging.error(f"{error} occurred while processing template {index}")
            finally:
                if events_buffer and cd.memory_usage() > cli_args.memory_limit:
                    with open(output, 'a+b') as file:
                        logging.info(f"Flushing {len(events_buffer)} events to {file.name}")
                        writer(file, schema, events_buffer, codec='snappy')
                    events_buffer.clear()
    if events_buffer:
        with open(output, 'a+b') as file:
            logging.info(f"Writing {len(events_buffer)} events to {file.name}")
            writer(file, schema, events_buffer, codec='snappy')
    toc = timer()
    logging.info(f"Elapsed time: {toc - tic:.2f} seconds.")
